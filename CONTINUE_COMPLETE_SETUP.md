# âœ… Continue - Complete Setup Guide

## Overview

Continue has two interfaces:
1. **VS Code Extension** - Code editing with AI
2. **CLI Tool** - Command-line AI assistant

Both are now configured to use **Ollama** (no API keys needed).

---

## ğŸš€ Quick Start (10 minutes)

### Step 1: Install Ollama (5 min)
```bash
yay -S ollama
# Or download from https://ollama.ai
```

### Step 2: Start Ollama (Keep Running)
```bash
ollama serve
```

### Step 3: Download Model (2-5 min)
In a **new terminal**:
```bash
ollama pull llama2
```

### Step 4: Use Continue

**VS Code Extension**:
- Press `Ctrl+L` to chat
- Press `Ctrl+K` to edit code

**CLI Tool**:
- Run `cn` to start
- Type your questions

---

## ğŸ“‹ Configuration

**File**: `~/.continue/config.json`

**Current Setup**:
```json
{
  "models": [
    {
      "title": "Ollama Local",
      "provider": "ollama",
      "model": "llama2",
      "apiBase": "http://localhost:11434"
    }
  ]
}
```

**Status**: âœ… Ready to use

---

## ğŸ¯ VS Code Extension

### Keyboard Shortcuts

| Action | Shortcut |
|--------|----------|
| Chat | Ctrl+L |
| Edit Code | Ctrl+K |
| Terminal | Ctrl+Shift+` |

### Usage Examples

**Chat**:
```
Ctrl+L
"Explain the quantum phase 13 implementation"
```

**Edit Code**:
```
Highlight code
Ctrl+K
"Add error handling"
```

**With Context**:
```
"@codebase How does QAOA work?"
"@file src/quantum/qaoa.py What does this do?"
```

---

## ğŸ’» CLI Tool

### Installation

```bash
npm i -g @continuedev/cli
```

### Login

```bash
cn login
```

### Usage

```bash
# Start CLI
cn

# Chat
â— Hello, what can you do?

# Use codebase context
â— @codebase How does QAOA work?

# Reference a file
â— @file src/quantum/qaoa.py Explain this

# Shell commands
! npm run build

# Exit
exit
```

### Keyboard Shortcuts

| Action | Shortcut |
|--------|----------|
| Send | Enter |
| New Line | Shift+Enter |
| Exit | Ctrl+C or `exit` |

---

## âœ¨ Features

âœ… **Chat with AI** - Ask questions about your code
âœ… **Code Editing** - Get AI suggestions for code changes
âœ… **Codebase Search** - Use `@codebase` to search your project
âœ… **File References** - Use `@file` to reference specific files
âœ… **Terminal Integration** - Run shell commands
âœ… **No API Keys** - Uses local Ollama
âœ… **No Authentication** - Works offline
âœ… **Completely Free** - No costs

---

## ğŸ†˜ Troubleshooting

### "Failed to connect to Ollama"
```bash
# Make sure Ollama is running
ollama serve

# Verify it's working
curl http://localhost:11434/api/tags
```

### "Model not found"
```bash
# Download a model
ollama pull llama2

# List available models
ollama list
```

### "Slow responses"
Try a faster model:
```bash
ollama pull mistral
```

### "Still getting errors"
1. Close VS Code completely
2. Restart Ollama: `ollama serve`
3. Try again

---

## ğŸ“š Available Models

### Recommended
- **llama2** (7B) - Good balance of speed and quality
- **mistral** (7B) - Very fast

### Optimized for Code
- **codellama** (7B) - Specialized for coding

### For Chat
- **neural-chat** (7B) - Good for conversations

---

## ğŸ”§ Configuration Files

**Global Config** (Used by both VS Code and CLI):
- Location: `~/.continue/config.json`
- Status: âœ… Configured for Ollama

**Project Config** (For reference):
- Location: `/root/Qallow/.continue/config.json`
- Status: âœ… Also configured for Ollama

---

## ğŸ“Š Comparison

| Feature | VS Code | CLI |
|---------|---------|-----|
| Chat | âœ… | âœ… |
| Code Editing | âœ… | âŒ |
| Terminal | âœ… | âœ… |
| Codebase Search | âœ… | âœ… |
| File References | âœ… | âœ… |

---

## ğŸ¯ Next Steps

1. **Start Ollama**: `ollama serve`
2. **Download Model**: `ollama pull llama2`
3. **Use VS Code**: Press `Ctrl+L` to chat
4. **Or Use CLI**: Run `cn` to start

---

## âœ… Status

- **Configuration**: âœ… Complete
- **VS Code Extension**: âœ… Ready
- **CLI Tool**: âœ… Ready
- **Ollama Setup**: âœ… Configured
- **API Keys**: âœ… Not needed
- **Ready to Use**: âœ… YES

---

**Next**: Start Ollama with `ollama serve`

